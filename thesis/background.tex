\chapter{Programming language principles}
\label{sec:prog_lang_principles}

This chapter is intended to provide a primer to the theory of functional programming and programming languages, as relevant to this work on Hazel. The work performed for this thesis is concerned with the dynamic semantics of Hazel.

\Cref{sec:prog-lang-def} is concerned with explaining the notation used throughout this paper to describe formal systems. \Cref{sec:fp-lc} is concerned with incrementally building up the conceptual foundation for the gradually-typed $\lambda$-calculus, from which Hazel heavily borrows. In this section, the syntax and static semantics of the $\lambda$-calculus are explored; even though not directly tied to this work concerned with dynamics, these are critical to understanding the Hazel system. Much of the material presented in this section is standard material in a introductory text in programming language theory such as \cite{harper2016practical}. \Cref{sec:interpreters} provides some detail on different types of programming language implementations, which is standard material in an introductory compilers text such as \cite{aho86}. In particular, this section sheds some light on the rationale behind switching from an evaluation model based on substitution to an evaluation model based on environments, which forms the basis for a large part of this thesis. \Cref{sec:prog-intf} provides an overview of relevant topics in programming language interfaces.

\section{Specifications of programming languages}
\label{sec:prog-lang-def}

To be able to rigorously work with programming languages, as with any mathematical activity, we need to precisely define the behavior of programming languages that serve as our interface to computation. The definition (or specification) of a programming language is typically given as the combination of its \textit{syntax} and \textit{semantics}, which will be discussed below.

Note that the specification of a programming language is orthogonal to its \textit{implementation(s)}; a programming language may have several implementations, which may have differing support for language features and different performance characteristics. Common classifications of programming language implementations are discussed in \Cref{sec:comp-vs-interp}.

\subsection{Syntax}
\label{sec:syntax}

The syntax of a programming language is defined by a grammar. The grammar described in Hazelnut \cite{conf/popl/Hazelnut17} is reproduced in \Cref{fig:sample-grammar} as an example.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{align*}
        \tau &::= \tau\to\tau
               \mid\tnum
               \mid\hehole \\
        e &::= x
            \mid\lambda x.e
            \mid e\ e
            \mid e+e
            \mid e:\tau
            \mid\hehole
            \mid\hhole{e}
      \end{align*}
    \end{singlespace}
  \end{mdframed}
  \caption{A sample grammar}
  \label{fig:sample-grammar}
\end{figure}

In this simple grammar, we have two productions: types and expressions. A type may have one of three forms: the $\tnum$ type, an arrow (function) type, or the hole type (similar to the $\gtlch$ type from the GTLC described in \Cref{sec:gradual}). An expression may be a variable, a $\lambda$-abstraction\footnote{This may have several names depending on the context and programming language, such as: $\lambda$-function, function literal, arrow function, anonymous function, or simply function. $\lambda$-abstractions are unary by definition -- higher-arity functions may be constructed via \textit{function currying}.}, a primitive binary operation, a type ascription, an empty hole, or a non-empty hole. Hole expressions and the hole type are specific to Hazel. Parentheses are not shown in this grammar; they are optional except to specify order of operations.

Parts of this grammar will be revisited when discussing the $\lambda$-calculus described in \Cref{sec:lambda-calculus}, and when discussing Hazel's grammar described in \Cref{sec:hazel}. In particular, this Hazelnut grammar is a superset of the grammar of the GTLC described in \Cref{sec:gradual}, and a subset of the grammar in the implementation of Hazel, which includes additional forms such as \texttt{let}, \texttt{case}, and pair expressions. Some of these forms will be important cases for our study of evaluation.

Due to Hazelnut being a structural edit calculus (as described in \Cref{sec:structure-editors}), there is no need to worry about syntax errors. The syntax describes the external language of Hazel, which will be translated into the internal language via the elaboration algorithm prior to evaluation.

\subsection{Notation for semantics}
\label{sec:semantics-notation}
In formal logic, a standard notation for \textit{rules of inference} is shown in \Cref{fig:sample-inference-rule}. $p_1,p_2,\dots,p_n$ are the \textit{antecedents} (alternatively, \textit{premises}) and $q$ is the single \textit{consequent} (alternatively, \textit{conclusion}). Each of $p_1,p_2,\dots,p_n,q$ is a \textit{judgment} (alternatively, \textit{proposition} or \textit{statement}). We may pronounce this rule as ``if all of $p_1,p_2,\dots,p_n$ are true, then $q$ must be true.'' Note that the antecedent of the rule is the logical conjunction of the antecedants $\bigwedge_{i=1}^n p_i$. The logical disjunction of antecedants $\bigvee_{i=1}^n p_i$ is expressed by writing separate rules with the same consequent. A rule with zero premises is an \textit{axiom}, i.e., the conclusion is vacuously true. We may build up a formal logic system using such rules. Note that the set of judgments that form the consequents of a rule, as well as the set of rules in a formal system, are both unordered; however, any computer program that carries out these judgments must choose some order in which to evaluate the set of antecedents or the order in which to evaluate a set of equally-viable rules.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{equation*}
        \Infer{RuleName}{p_1 \\ p_2 \\ \dots \\ p_n}{q}
      \end{equation*}
    \end{singlespace}
  \end{mdframed}
  \caption{Notation for an inference rule}
  \label{fig:sample-inference-rule}
\end{figure}

Each new judgment form will be introduced annotated with the modes of each term. For example, the typing judgment $\Gamma^-\vdash e^-:\tau^+$ indicates that $\Gamma$ and $e$ are inputs to the judgment and $\tau$ is an output of the judgment. If there are no terms with output mode in a judgment, then the ability to logically construct the judgment is its sole (boolean) output.

A derivation (proof) of a judgment is shown by chaining together inference rules, such that the final consequent is the statement to be proved. We may visualize a derivation as a tree rooted at the judgment to be proved, and whose children are (recursively) the antecedent judgments; the leaf nodes of this tree must be axioms.

To ensure that the system of inference rules covers the entire semantics of a language, to ensure that rules to not conflict, and to ensure that rules give the language the desired behavior, we may establish \textit{metatheorems}. Metatheorems are intuitive, high-level invariants or properties that describe the behavior of the overall system. We prove the correctness of an implementation by proving that the metatheorems are upheld by the inference rules. In the foundational papers for Hazel's core semantics \cite{conf/popl/Hazelnut17,conf/popl/HazelnutLive19}, metatheorems are amply used to justify and verify the correctness of the rules. Agda \cite{bove2009brief}, an interactive proof checker and dependently-typed programming language, is used for these proofs \cite{agda2017,agda-dynamics}.

\subsection{Static semantics}
\label{sec:static-semantics}

The \textit{static semantics} of a programming language describes properties of a program that can be checked prior to program evaluation. Static semantics typically refers primarily to \textit{type checking}. In Hazelnut Live, we have the process of \textit{elaboration} that transforms the \textit{external language} (a program expressed in the syntax of Hazel) to the \textit{internal language} (an intermediate representation more amenable to evaluation), which occurs before evaluation and incorporates the type checking rules. Elaboration and the internal language will be discussed further in \Cref{sec:comp-vs-interp}. The type checking and elaboration algorithms form the static semantics of Hazel.

It is formative to provide an overview of type checking. While the static semantics is not very important to the core work in this thesis, a fundamental understanding is key to understanding the motivation and bidirectionally-typed action calculus behind Hazel, as well as understanding the formulation of gradual typing described in \Cref{sec:gradual}.

The \textit{typing judgment} $\Gamma^-\vdash e^-:\tau^+$ states that, with respect to the typing context $\Gamma$, the expression $e$ is well-typed with type $\tau$. The typing context is a set of variable typing judgments $\{x:\tau\}$. A few sample typing judgments are shown in \Cref{fig:sample-typing-rules}.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{mathpar}
        \Infer{TNum}{}{\Gamma\vdash\hnum{n}:\tnum}
        \and
        \Infer{TVar}{}{\Gamma,x:\tau\vdash x:\tau}
        \and
        \Infer{TAnnArr}{
          \Gamma,x:\tau_1\vdash e:\tau_2
        }{\Gamma\vdash(\lambda x:\tau_1.e):\tau_1\to\tau_2}
        \and
        \Infer{TAp}{
          \Gamma\vdash e_2:\tau_1 \\
          \Gamma\vdash e_1:\tau_1\to\tau_2
        }{\Gamma\vdash e_1\ e_2:\tau_2}
      \end{mathpar}
    \end{singlespace}
  \end{mdframed}
  \caption{Sample typing rules}
  \label{fig:sample-typing-rules}
\end{figure}

There are a few noteworthy items here. The syntax $\Gamma,x:\tau$ indicates the typing context $\Gamma$ extended with the binding $x:\tau$. Thus, when it is part of the consequent, it means that we are stating the typing judgment with respect to a different typing context $\Gamma'=\Gamma,x:\tau$. The type of a number is always $\tnum$. The type of a variable may only be determined if its type exists in the typing context (which, according to this limited set of rules, may only be extended during a function application). Lambda expressions can only be typed if they are fully-annotated: i.e., if the argument's type is annotated and the body is also assigned a type. This example typing system is very minimal and not practical for larger systems: every $\lambda$-abstraction would have to be typed for the entire expression to be well-typed. Consider even the simple example $(\lambda x.x)\ 2$, which cannot be typed according to the simple system above due to the unannotated $\lambda$-abstraction.

A type system that allows for fewer type annotations, while remaining reasonably simple to formulate and implement, is \textit{bidirectional typing} \cite{Dunfield_2022,chlipala2005strict,pierce2000local}, or \textit{local type inference}. Bidirectional typing involves two typing judgments: the \textit{synthetic type judgment} $\Gamma^-\vdash e^-\Rightarrow\tau^+$ (pronounced ``given typing context $\Gamma$, expression $e$ synthesizes type $\tau$), and the \textit{analytic type judgment} $\Gamma^-\vdash e^-\Leftarrow\tau^-$ (pronounced ``given typing context $\Gamma$, expression $e$ analyzes against type $\tau$). The synthetic type judgment outputs a type (the exact or ``narrowest'' type of the expression), whereas the analytic type judgment takes a type as an input and ``checks'' the expression against that (``wider'') type. With these two judgments, we loosen the antecedant judgments when synthesizing a type. We re-express the above type assignment system into a similar bidirectional type system, shown in \Cref{fig:bidirectional-typing}.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{mathpar}
        \Infer{TSynNum}{}{\Gamma\vdash\hnum{n}\Rightarrow\tnum}
        \and
        \Infer{TSynVar}{}{\Gamma,x:\tau\vdash x\Rightarrow\tau}
        \and
        \Infer{TSynAnnArr}{
          \Gamma,x:\tau_1\vdash e\Rightarrow\tau_2
        }{\Gamma\vdash(\lambda x:\tau_1.e)\Rightarrow\tau_1\to\tau_2}
        \and
        \Infer{TSynAp}{
          \Gamma\vdash e_2\Rightarrow\tau_1 \\
          \Gamma\vdash e_1\Leftarrow\tau_1\to\tau_2
        }{\Gamma\vdash e_1\ e_2\Rightarrow\tau_2}
        \and
        \Infer{TAnaArr}{
          \Gamma,x:\tau_1\vdash e\Leftarrow\tau_2
        }{\Gamma\vdash\lambda x.e\Leftarrow\tau_1\to\tau_2}
        \and
        \Infer{TAnaSubsume}{
          \Gamma\vdash e\Rightarrow\tau
        }{\Gamma\vdash e\Leftarrow\tau}
      \end{mathpar}
    \end{singlespace}
  \end{mdframed}
  \caption{Simple bidirectional type system}
  \label{fig:bidirectional-typing}
\end{figure}

Now, we may synthesize the type of $(\lambda x.x)\ 2$; the derivation uses all of the rules above. Note the presence of the last rule; \textit{subsumption} states that an expression analyzes against its synthesized type, which should fit the earlier intuition of type synthesis producing the ``narrowest'' type and type analysis checking against a ``wider'' type. Subsumption allows us to avoid manually writing type analysis rules for most types.

Algorithmically, bidirectional typing begins by synthesizing the type of the top-level expression; if it successfully synthesizes, then the expression is well-typed. A more complete discussion of bidirectional typing is left to Dunfield \cite{Dunfield_2022}, who provides an overview of bidirectional typing, or to the formulation of Hazel's bidirectional typing \cite{conf/popl/Hazelnut17}. Hazelnut is at its core a bidirectionally-typed ``edit calculus'' \cite{conf/popl/Hazelnut17}, citing the balance of usability and simplicity of implementation.

The elaboration algorithm is bidirectionally-typed and fairly specific to Hazel and described in \Cref{sec:hazel-dynamics}. It is based off of the cast calculus from the GTLC.

More advanced type inference algorithms such as type unification are used in the highly advanced type systems of languages such as Haskell \cite{gundry2013type}, and are out of scope for this work.

\subsection{Dynamic semantics}
\label{sec:dynamic-semantics}

The \textit{dynamic semantics} (alternatively, \textit{evaluation semantics}) of a programming language describes the evaluation process. Evaluation is the algorithmic reduction of an expression to a \textit{value}, an irreducible expression.

The style of rules that are used to define the dynamic semantics of a programming language are called \textit{operational semantics}, because they model the operation of a computer when compiling or evaluating a programming language. There are two major styles of operational semantics.

The first of these styles is \textit{structural operational semantics} as introduced by Plotkin \cite{plotkin1981structural} (alternatively, \textit{small-step semantics}). In the small-step semantics, the \textit{evaluation judgment} is $e_1^-\to e_2^+$, where $e_1$ and $e_2$ are expressions in the language.

For example, let us describe the dynamic semantics of an addition operation using a small-step semantics. This is described using the three rules shown in \Cref{fig:small-step-addition}.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{mathpar}
        \Infer{EPlus$_1$-Small}{e_1\to e_1'}{e_1+e_2\to e_1'+e_2}
        \and
        \Infer{EPlus$_2$-Small}{e_2\to e_2'}{\hnum{n_1}+e_2\to \hnum{n_1}+e_2'}
        \and
        \Infer{EPlus$_3$-Small}{}{\hnum{n_1}+\hnum{n_2}\to\hnum{n_1+n_2}}
      \end{mathpar}
    \end{singlespace}
  \end{mdframed}
  \caption{Small-step dynamic semantics for addition}
  \label{fig:small-step-addition}
\end{figure}

The algorithm carries itself out as follows: while $e_1$ is reducible, reduce it using some applicable evaluation rule. Once $e_1$ becomes a value, the first rule is no longer applicable (as $e_1$ cannot further reduce) and $e_2$ reduces until it too is a value. Finally, the third rule is applicable, and reduces the expression down to a single number literal. Note that if either $e_1$ or $e_2$ do not reduce down to a number literal, then the expression will not evaluate fully; this kind of failure cannot happen in a strongly-typed language due to typing rules.

The second of these styles is \textit{natural operational semantics} as introduced by Kahn \cite{Kahn1987NaturalS} (alternatively, \textit{big-step semantics}). In the big-step semantics, the evaluation judgment is $e^-\Downarrow v^+$, where $e$ is an expression in the language, and $v\textsf{ value}$.

To express the evaluation of addition in the big-step semantics, we need only a single rule, shown in \Cref{fig:big-step-addition}. In this case, the antecedants indicate that the subexpressions must be recursively evaluated, but (as noted earlier) this notably doesn't specify the order of evaluation of the antecedants, unlike the small-step notation.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{mathpar}
        \Infer{EPlus-B}{
          e_1\Downarrow\hnum{n_1} \\
          e_2\Downarrow\hnum{n_2} \\
        }{e_1+e_2\Downarrow\hnum{n_1+n_2}}
      \end{mathpar}
    \end{singlespace}
  \end{mdframed}
  \caption{Big-step dynamic semantics for addition}
  \label{fig:big-step-addition}
\end{figure}

In the big-step notation, values are distinguished by the judgment $v\Downarrow v$; i.e., values evaluate to themselves. In the small-step notation, there will be no applicable rule to further reduce a value. Following the notation from \cite{conf/popl/Hazelnut17,conf/popl/HazelnutLive19}, we can alternatively write this using the equivalent judgment $v^-\textsf{ value}$. In the stereotypical untyped $\lambda$-calculus, the only values are $\lambda$-abstractions. We can denote this using the axiom in \Cref{fig:value-judgment}.

\begin{figure}
  \centering
  \begin{mdframed}
    \begin{singlespace}
      \begin{mathpar}
        \Infer{VLam}{}{\lambda x.e\textsf{ value}}
      \end{mathpar}
    \end{singlespace}
  \end{mdframed}
  \caption{Values in \ulc}
  \label{fig:value-judgment}
\end{figure}

In Hazel, we have other base types such as integers, floats, and booleans, which also have axiomatic $\textsf{value}$ judgments. For composite data such as pairs or injections (binary sum type constructors), the expression is a value iff its subexpression(s) are values.

The implementation of an evaluator with a program stepper capability (as is commonly found in debugger tools) is more amenable to implementation using a small-step operational semantics, since it precisely details the sub-reductions when evaluating an expression. The evaluation semantics of Hazelnut Live are originally described using a small-step semantics in \cite{conf/popl/HazelnutLive19}. To simplify the rules, the concept of an \textit{evaluation context} $\mathcal{E}$ is used to recurse through subexpressions.

The big-step semantics is often simpler because it involves fewer rules, and is more efficient to implement. As a result, the implementation of evaluation in Hazel more closely follows the big-step semantics, and it is the notation used predominantly throughout this work.

\section{Introduction to functional programming and the $\lambda$-calculus}
\label{sec:fp-lc}

To understand this work on extending Hazel's dynamic semantics, one must have a satisfactory understanding of Hazel. Understanding Hazel requires some understanding of the \textit{functional programming paradigm}, as Hazel is a stereotypical functional language. 

Functional programming \cite{hughes1989functional} is a programming paradigm that is highly involved with function application, function composition, and first-class functions. It is a subclass of the declarative programming paradigm, which is concerned with pure\footnote{``Pure'' in the sense of a pure function, i.e., without mutable state or side-effects.} expression-based computation. Declarative programming is often considered the complement of imperative programming, which may be characterized as programming with mutable state, side effects, or statements. Purely functional programming is a subset of functional programming that deals solely with pure functions; non-pure languages may allow varying degrees of mutable state but typically encourage the use of pure functions.

Functional languages are based on Alonzo Church's $\lambda$ calculus \cite{harper2016practical} as its core evaluation and typing semantics, which provides a minimal foundation for computation. The syntax of functional programming languages is based off the $\lambda$ calculus. This, along with the lack of mutable state and side effects, allows functional programming to be easily mathematically modeled and reasoned about, making it particularly amenable to proofs about programming languages. This is as opposed to in imperative programming, in which the mutable ``memory cell'' interpretation of variables and side-effects complicates formalizations. A number of programming languages incorporate both functional and imperative language features, such as Hazel's implementation language OCaml \cite{hickey2014real}. These languages are classified as multi-paradigm programming languages.

Hazelnut's core calculus is heavily based on the \textit{gradually-typed $\lambda$-calculus} (GTLC) introduced by Siek \cite{Siek06gradualtyping,siek2015refined}. This itself is an extension of the simply-typed $\lambda$-calculus (STLC), which is an extension of the untyped $\lambda$-calculus (ULC), the simplest implementation of Church's $\lambda$-calculus. The STLC, the untyped $\lambda$-calculus, and Church's $\lambda$-calculus are standard textbook material in programming language theory \cite{harper2016practical}. We provide a brief self-contained introduction to these formalizations in \Cref{sec:lambda-calculus-primer}. This will form the foundational understanding for the study of dynamic semantics in this work, as well as a general understanding of Hazel.

\section{Implementations of programming languages}
\label{sec:interpreters}

In order to run programs in a programming language on a computer, we must have an \textit{implementation} of the language. Hazel is implemented as an interpreted language, whose runtime is transpiled to Javascript so that it may be run as a client-side web application in the browser.

It is important to note that the definition of a language (its syntax and semantics) are largely orthogonal to its implementation. In other words, a programming language does not dictate whether it requires a compiler or interpreter implementation, and languages sometimes have multiple implementations.

\subsection{Compiler vs. interpreter implementations}
\label{sec:comp-vs-interp}

There are two general classes of programming language implementations: \textit{interpreters} and \textit{compilers} \cite{aho86}. Both types of implementations share the function of taking a program as input, and should be able to produce the same result (assuming an equal and determinstic machine state, equal inputs, correct implementations, and no exceptional behavior due to differences in resource usage).

A compiler is a programming language implementation that converts the program to some low-level representation that is natively executable on the hardware architecture (e.g., x86-64 assembly for most modern personal computers, or the virtualized JVM architecture) before evaluation. This process typically comprises \textit{lexing} (breaking down into atomic tokens) the program text, \textit{parsing} the lexed tokens into a suitable \textit{intermediate representation} (IR) such as LLVM, performing optimization passes on the intermediate representation, and then generating the target bytecode (such as x86-64 assembly) \cite{aho86}. The bytecode outputted from the compilation process is used for evaluation. Compiled implementations tend to produce better runtime efficiency, since the compilation steps are performed separate of the evaluation, and because there is little to no runtime overhead.

An interpreter is a programming language implementation that does not compile down to native bytecode, and thus requires an interpreter or \textit{runtime}, which performs the evaluation. Interpreters still require lexing and parsing, and may have any number of optimization stages, but do not generate bytecode for the native machine, instead evaluating the program directly.

The term \textit{elaboration} \cite{harper2000type} may be used to describe the process of transforming the \textit{external language} (a well-formed, textual program) into the \textit{internal language}. The interior language may include additional information not present in the external language, such as types generated by type inference or bidirectional typing.

The distinction between compiled and interpreted languages may not be very clear: some implementations feature just-in-time (JIT) compilation that allow ``on-the-fly'' compilation (e.g., common implementations of the JVM and CLR \cite{sestoft2002runtime}), and some implementations may perform the lexing and parsing separately to generate a non-native bytecode representation to be later evaluated by a runtime. A general characterization of compiled vs. interpreted languages is the amount of runtime overhead required by the implementation.

Hazel is a purely interpreted language implementation, as optimizations for speed are not among its main concerns. However, performance is clearly one of the main concerns of this thesis project, but the gains will be algorithmic and use the nature of Hazel's structural editing and hole calculus to benefit performance, rather than changing the fundamental implementation. There is, however, a separate endeavor to write a compiled interpretation of Hazel \cite{hazelc}, which is outside the scope of this project.

\subsection{The substitution and environment models of evaluation}
\label{sec:sub-vs-eval}

Evaluation in Hazel was originally performed using a \textit{substitution model of evaluation}, which is a theoretically simpler model. In this model, variables that are bound by some construct are substituted into the construct's body. For example, the variable(s) bound using a \mintinline{ocaml}|let|-expression pattern are substituted in the \mintinline{ocaml}|let|-expression's body, and the variable(s) bound during a function application are substituted into the function's body, and then the body is evaluated.

In this formulation, variables are ``given meaning'' via substitution; once evaluation reaches an expression, all variables in scope (in the typing context) will have been replaced by their value by some containing binding expression. In other words, variables are eagerly substituted with their value when bound. The substitution model is useful for teaching purposes because it is simple and close to its mathematical definition: a variable can be thought of as an equivalent stand-in for its value.

However, for the purpose of computational efficiency, a model in which values are lazily expanded (``looked-up'') only when needed is more efficient. Variables are lazily looked up in their environment. This is called the \textit{environment model of evaluation}, and generally is more efficient because the runtime does not need to perform an extra substitution pass over subexpressions and because untraversed (unevaluated) branches do not require substituting. Lastly, the runtime does not need to carry an expression-level IR of the language, due to the fact that the substitution model manipulates expressions, while evaluation does not. This means that the latter is more amenable for compilation, and is how compiled languages tend to be implemented: each frame of the theoretical stack frame is a de facto environment frame. While switching from the substitution to environment model is not an improvement in asymptotic efficency, these effects are useful especially for high-performance and compiled languages.

Note that the use of substitution or environments for evaluation is orthogonal to strict (applicative-order) or lazy (normal-order) evaluation \cite{plotkin1975call}. The use of substitution or environments refers to the eager or lazy substitution of variables. Strict and lazy evaluation refer to when the expression bound to a variable is evaluated.

\section{Approaches to programming interfaces}
\label{sec:prog-intf}

The most traditional programming interface is the text source file. In this case, we describe two innovations on plaintext files that are relevant to Hazel's design and use cases.

\subsection{Structure editors}
\label{sec:structure-editors}

\textit{Structure editors} form a class of programming language editors that allows one to directly interface with the \textit{abstract syntax tree} of a programming language via a restricted set of \textit{edit actions}, rather than via the manipulation of unstructured text. The immediate benefit of this is the elimination of the entire class of errors related to syntax.

A major use case for structural editing is for the purpose of programming education. By eliminating syntactic errors, the student may shift their attention more towards semantic issues in their code. For example, Carnegie Mellon University developed a series of structural editors (GNOME, MacGnome, and ACSE)  targeted at programming education \cite{miller1994evolution}. Scratch is a graphical structural editor targeted at younger students (aged 8-16) developed at MIT \cite{maloney2010scratch}. Programming education is one of the main proposed use cases for Hazel, such as its use in Hazel Tutor \cite{potter2020hazel}.

Structure editors are not limited to programming education. mbeddr is a structure editor for embedded programming \cite{voelter2012mbeddr}, and Lamdu is a structure editor for a Haskell-like functional programming language \cite{lotem_chuchem}.

One of the major drawbacks of structural editing is the decrease in usability by restricting the set of edit actions. The degree to which an editor ``resists local changes'' is a property known as \textit{viscosity} \cite{green1989cognitive}. Structural and visual editing is expectedly more viscous than unstructured plaintext editing. Reducing the viscosity of structural editing is not a goal of Hazel, but a related project at the University of Michigan, Tylr \cite{tylr}, tackles the problem of editing viscosity and may make its way into future versions of Hazel.

Omar et al. \cite{conf/popl/Hazelnut17} describes several other structure editors and their relation to Hazel, and in particular other structure editors which also attempt to maintain well-typedness or operate on formal definitions of an underlying language.

\subsection{Live programming environments and computational notebooks}
\label{sec:notebook-editors}

Burckhardt et al. describes live programming environments as providing continuous feedback that narrows the ``temporal and perceptive gap between program development and code execution''
 \cite{10.1145/2499370.2462170}. A common example of a live programming environment are read-evaluate-print loops (REPLs), which allow line-by-line evaluation of expressions. Computational notebooks form an example of live programming environments.

Computational notebooks, such as in IPython/Jupyter Notebook \cite{perez2007ipython} or MATLAB, is another trend in programming languages that has been popular in scientific applications. They provide much feedback about program's dynamic state, especially interactively or graphically. Notebook-style editing allows one to intersperse editing and evaluation of a program. Programs may be run in sections (potentially out of order), maintaining state between sections evaluations -- this is typically for efficiency reasons. There is a large design space in current computational notebooks, with many possible variations in code evaluation, editing semantics, and displaying notebook outputs \cite{lau2020design}.

Hazel may be considered a live editor as it attempts to eliminate the feedback gap, by providing static and dynamic feedback throughout the lifetime of then program. The fill-and-resume functionality described in \cite{conf/popl/HazelnutLive19} and implemented in this work provide a novel possible implementation of notebook-like partial evaluation.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
