\chapter{Programming language principles}
\label{sec:prog_lang_principles}

This chapter is intended to provide a primer to the theory of functional programming and programming languages, as relevant to this work on Hazel.

\section{Specifications of programming languages}
\label{sec:prog-lang-def}

To be able to rigorously work with programming languages, as like any mathematical activity, we need to be able to precisely define the behavior of programming languages that serve as our interface to computation. We typically define the definition (or specification) of a programming language as the combination of its \textit{syntax} and \textit{semantics}, which will be discussed below.

Note that the specification of a programming language is orthogonal to its \textit{implementation(s)}; a single programming language may have several implementations of the specification, which may have differing support for language features and different performance characteristics. Common classifications of programming language implementations are discussed in \Cref{sec:comp-vs-interp}.

\subsection{Syntax}
\label{sec:syntax}

\todo{this section}

\subsection{Notation for semantics}
\label{sec:semantics-notation}
In formal logic, a standard notation for \textit{rules of inference} is shown below.
\begin{singlespace}
  \[\Infer{SampleRule}{p_1 \\ p_2 \\ \dots \\ p_n}{q}\]
\end{singlespace}
$p_1,p_2,\dots,p_n$ are the \textit{antecedents} (alternatively, \textit{premises}) and $q$ is the (single) \textit{consequent} (alternatively, \textit{conclusion}). Each of $p_1,p_2,\dots,p_n,q$ is a \textit{judgment} (alternatively, \textit{proposition} or \textit{statement}); we may interpret the rule as: ``if all of $p_1,p_2,\dots,p_n$ are true, then $q$ must be true'' (the antecedent of the rule is the logical conjunction of the antecedants $\bigwedge_{i=1}^n p_i$). The logical disjunction of antecedants $\bigvee_{i=1}^n p_i$ is expressed by writing separate rules with the same consequent. A rule with zero premises is an \textit{axiom}, i.e., the conclusion is vacuously true. We may build up a formal logic system using such rules; in our case the formal specification of the static and dynamic semantics of a programming language. Note that the set of judgments that form the consequents of a rule, as well as the set of rules in a formal system, are both unordered; however, any computer program that carries out these judgments must choose some order in which to evaluate the set of antecedents or the order in which to evaluate a set of equally-viable rules.

A derivation of a statement in such a system by chaining together inference rules, such that the final consequent is the statement to be proved.

\todo{example of a derivation in a simple fabricated system}

To ensure that the system of inference rules covers the entire semantics of a language, to ensure that rules to not conflict, and to ensure that rules give the language the desired behavior, we may establish additional metatheorems, which must be proved for all of the inference rules in the logic system. In the foundational papers for Hazel's core semantics \cite{conf/popl/Hazelnut17,conf/popl/HazelnutLive19}, metatheorems are amply used to justify and verify the correctness of the rules. Agda \cite{bove2009brief}, an interactive proof checker and dependently-typed programming language, is used for these proofs \cite{agda2017,agda-dynamics}.

\todo{example of a metatheorem (from the original papers?)}

\subsection{Static semantics}
\label{sec:static-semantics}

\subsection{Dynamic semantics}
\label{sec:dynamic-semantics}

The \textit{dynamic semantics} (alternatively, \textit{evaluation semantics}) of a programming language describes the evaluation process. Evaluation is the algorithmic reduction of an language expression to a \textit{value}, an irreducible expression.

Let us consider values in some more detail. Values are distinguished by the judgment $v\Downarrow v$; i.e., values evaluate to themselves. Following the notation from \cite{conf/popl/Hazelnut17,conf/popl/HazelnutLive19}, we can alternatively write this using the equivalent judgment $v\textsf{ value}$. In the stereotypical untyped $\lambda$-calculus, the only values are $\lambda$-abstractions. We can denote this using the axiom:

\begin{singlespace}
  \begin{mathpar}
    \Infer{Val-$\lambda$}{}{\lambda x.e\textsf{ value}}
  \end{mathpar}
\end{singlespace}

In Hazel, we have other base types such as integers, floats, and booleans, which also have axiomatic $\textsf{value}$ judgments. For composite data such as pairs or injections (binary sum type constructors), the expression is a value iff its subexpression(s) are values.

The rules that are used to define the dynamic semantics of a programming language are called \textit{operational semantics}, because they model the operation of a computer when compiling or evaluating a programming language. There are two major styles of operational semantics.

The first of these styles is \textit{structural operational semantics} as introduced by Plotkin \cite{plotkin1981structural} (alternatively, \textit{small-step semantics}). In the small-step semantics, the judgments are of the form $e_1\to e_2$, where $e_1$ and $e_2$ are expressions in the language, and $\to$ is the operation being defined.

For example, let us describe the dynamic semantics of an addition operation using a small-step semantics. This is described using the following three rules:

\begin{singlespace}
  \begin{mathpar}
    \Infer{EvalS-Plus$_1$}{e_1\to e_1'}{e_1+e_2\to e_1'+e_2}
    \and
    \Infer{EvalS-Plus$_2$}{e_2\to e_2'}{\hnum{n_1}+e_2\to \hnum{n_1}+e_2'}
    \and
    \Infer{EvalS-Plus$_3$}{}{\hnum{n_1}+\hnum{n_2}\to\hnum{n_1+n_2}}
  \end{mathpar}
\end{singlespace}

The algorithm carries itself out as follows: while $e_1$ is reducible, reduce it using some applicable evaluation rule. Once $e_1$ becomes a value, the first rule is no longer applicable (as $e_1$ cannot further reduce) and $e_2$ reduces until it too is a value. Finally, the third rule is applicable, and reduces the expression down to a single number literal. Note that if either $e_1$ or $e_2$ do not reduce down to a number literal, then the expression will not evaluate fully; this kind of failure cannot happen in a strongly-typed language due to typing rules.

The second of these styles is \textit{natural operational semantics} as introduced by Kahn \cite{Kahn1987NaturalS} (alternatively, \textit{big-step semantics}). In the big-step semantics, the judgments are of the general form $e\Downarrow v$, where $e$ is an expression in the language, $\Downarrow$ represents some operation that is being defined, and $v\textsf{ value}$.

To express the evaluation of addition in the big-step semantics, we need only a single rule. In this case, the antecedants indicate that the subexpressions must be recursively evaluated, but (as noted earlier) this notably doesn't specify the order of evaluation of the antecedants, unlike the small-step notation.

\begin{singlespace}
  \begin{mathpar}
    \Infer{EvalB-Plus}{
      e_1\Downarrow\hnum{n_1} \\
      e_2\Downarrow\hnum{n_2} \\
    }{e_1+e_2\Downarrow\hnum{n_1+n_2}}
  \end{mathpar}
\end{singlespace}

The implementation of an evaluator with a program stepper capability (as is commonly found in programming language debuggers) is more amenable to implementation using a small-step operational semantics, since it precisely details the sub-reductions when evaluating an expression. The evaluation semantics of Hazelnut Live are originally described using a small-step semantics in \cite{conf/popl/HazelnutLive19}. To simplify the rules, the concept of an \textit{evaluation context} $\mathcal{E}$ is used to recurse through subexpressions.

The big-step semantics is often simpler because it involves fewer rules, and is more efficient to implement. As a result, the implementation of evaluation in Hazel more closely follows the big-step semantics, and it is the notation used predominantly throughout this work.

% TODO: make this its own subsection
\subsection{Gradual typing}
\label{sec:gradual}


\section{Functional programming}
\label{sec:fp}

Functional programming \todoref{functional programming} is a programming paradigm that is highly involved with function application, function composition, and first-class functions. It is generally a subtype of, and often associated with, the declarative programming paradigm, which is concerned with expression-based computation, often without mutable state or side-effects. Declarative programming is often considered the complement of imperative programming, which may be characterized as programming with mutable state, side effects, or statements. Purely functional programming is a subset of functional programming that deals solely with pure functions; non-pure languages may allow varying degrees of mutable state but typically encourage the use of pure functions.

Functional languages are based on Alonzo Church's $\lambda$ calculus \todoref{lambda calculus} as its core evaluation and typing semantics, which provides a minimal foundation for computation. The syntax of functional programming languages is based off the $\lambda$ calculus. This, along with the lack of mutable state and side effects, allows functional programming to be easily mathematically modeled and reasoned about, making it particularly amenable to proofs about programming languages. This is as opposed to in imperative programming, in which the mutable ``memory cell'' interpretation of variables and side-effects complicates formalizations.

Hazel is one such (purely) functional programming languages. Other languages that are classified as functional include the ML family of languages, Haskell, Elm, and the LISP family of languages. Examples of imperative programming languages include C, C++, FORTRAN, Java, and Golang. A number of languages incorporate both functional and imperative styles, such as Javascript, Python, Scala, and Rust \todoref{all of these languages and their classifications}.

% TODO: show a simple example of programs in these paradigms

\subsection{The $\lambda$-calculus}
\label{sec:lambda-calculus}

\todo{this section is a mess at the moment; sorry}

The simplest form of the $\lambda$ calculus, the \textit{untyped $\lambda$ calculus}, comprises only three expression forms:
\begin{align}
  \label{eq:untyped-lambda-calculus}
  x\tag{Variable} \\
  \lambda x.e\tag{$\lambda$ abstraction} \\
  e_1\ e_2\tag{Function application}
\end{align}
where $e$, $e_1$, and $e_2$ are also expressions of one of these three forms. A $\lambda$ abstraction is also known as a $\lambda$ expression, $\lambda$ function, anonymous function, or simply function, and function application is also known as function invocation or $\beta$-reduction\footnote{A few notes for the imperative programmer: $\lambda$ functions only take a single parameter. A function of multiple parameters may be constructed as a series of recursive functions, each taking a single parameter -- a process known as \textit{currying}. Similarly, function application comprises two expressions, the first in \textit{function position} (which must evaluate to a $\lambda$ function), and the second in \textit{argument position}. Function application is traditionally (e.g., in the ML and Haskell families of programming languages, although LISP is an exception) denoted using the infix operator `` '' (space), which has the highest precedence of any infix operator; parentheses are only used to indicate order of operations and typically omitted when not necessary.}. The only reduction in the untyped $\lambda$ calculus is $\beta$-reduction, which is stated as follows.\[(\lambda x.e_1)\ e_2\to [e_1/x]e_2\] The notation $[x/y]z$ indicates substitution, and may be pronounced ``the substitution of $x$ for $y$ in $z$.'' According to computability theory, since the untyped $\lambda$ calculus supports general recursion (through the use of a fixpoint operator), it is Turing-complete.

While the untyped $\lambda$ calculus is Turing-complete and thus as expressive as any other Turing-complete programming language, this is far too tedious to be of any practical use. In this minimal foundation, we do not have base types such as integers or booleans\footnote{In the untyped $\lambda$ calculus, the only value type are $\lambda$ abstractions. Any other data type, such as the set of natural numbers or booleans, may be represented using $\lambda$ abstractions via \textit{Church encoding}.} or a typing system. The typical formulation of the \textit{simply-typed $\lambda$ calculus} extends the untyped $\lambda$ calculus with a (non-$\lambda$ abstraction) base type $b$ of type $B$, and a type $\tau\in $ TODO: working here; this is a mess

% TODO: describe these using syntax and semantics

% TODO: move syntax and semantics and notation to here

% TODO: describe typing rules

% TODO: for practical purposes, let and case expressions

% TODO: introduce gradually-typed lambda calculus

\subsection{Purity and statefulness}
\label{sec:purity}

\subsection{The ML family, Elm, and Hazel}
\label{sec:ml-fam}

% ADTs
% typing system

\section{Implementations for programming languages}
\label{sec:interpreters}

In order for a programming language to be practical, it must not only be defined as a set of syntax and semantics, but also have an \textit{implementation} to run programs in the language. Hazel is implemented as an interpreted language, whose runtime is transpiled to Javascript so that it may be run as a client-side web application in the browser.

It is important to note that the definition of a language (its syntax and semantics) are largely orthogonal to its implementation. In other words, a programming language does not dictate whether it requires a compiler or interpreter implementation, and languages sometimes have multiple implementations.

\section{Compiler vs. interpreter implementations}
\label{sec:comp-vs-interp}

There are two general classes of programming language implementations: \textit{interpreters} and \textit{compilers} \cite{aho86}. Both types of implementations share the function of taking a program as input, and should be able to produce the same result (assuming an equal and determinstic machine state, equal inputs, correct implementations, and no exceptional behavior due to differences in resource usage).

A compiler is a programming language implementation that converts the program to some low-level representation that is natively executable on the hardware architecture (e.g., x86-64 assembly for most modern personal computers, or the virtualized JVM architecture) before evaluation. This process typically comprises \textit{lexing} (breaking down into atomic tokens) the program text, \textit{parsing} the lexed tokens into a suitable \textit{intermediate representation} (IR) such as LLVM, performing optimization passes on the intermediate representation, and then generating the target bytecode (such as x86-64 assembly) \cite{aho86}. The bytecode outputted from the compilation process is used for evaluation. Compiled implementations tend to produce better runtime efficiency, since the compilation steps are performed separate of the evaluation, and because there is little to no runtime overhead.

An interpreter is a programming language implementation that does not compile down to native bytecode, and thus requires an interpreter or \textit{runtime}, which performs the evaluation. Interpreters still require lexing and parsing, and may have any number of optimization stages, but do not generate bytecode for the native machine, instead evaluating the program directly.

In certain contexts (especially in the ML spheres), the term \textit{elaboration} \cite{harper2000type} is used to the process of transforming the \textit{external language} (a well-formed, textual program) into the \textit{internal language} (IR). The interior language may include additional information not present in the external language, such as types generated by type inference or bidirectional typing.

The distinction between compiled and interpreted languages is not a very clear line: some implementations feature just-in-time (JIT) compilation that allow ``on-the-fly'' compilation (e.g., common implementations of the JVM and CLR \cite{sestoft2002runtime}), and some implementations may perform the lexing and parsing separately to generate a non-native bytecode representation to be later evaluated by a runtime. A general characterization of compiled vs. interpreted languages is the amount of runtime overhead required by the implementation.

Hazel is a purely interpreted language implementation, as optimizations for speed are not among its main concerns. However, performance is clearly one of the main concerns of this thesis project, but the gains will be algorithmic and use the nature of Hazel's structural editing and hole calculus to benefit performance, rather than changing the fundamental implementation. There is, however, a separate endeavor to write a compiled interpretation of Hazel \cite{hazelc}, which is outside the scope of this project.

\subsection{The substitution and environment models of evaluation}
\label{sec:sub-vs-eval}

% REF: https://cs.brown.edu/courses/cs173/2012/book/From_Substitution_to_Environments.html
% REF: https://www.cs.bham.ac.uk/research/projects/poplog/paradigms_lectures/lecture18.html
%   this introduces the issue with special forms like set! in a non-functional context

Evaluation in Hazel was originally performed using a \textit{substitution model of evaluation}, which is a theoretically simpler model. In this model, variables that are bound by some construct are substituted into the construct's body. For example, the variable(s) bound using a \mintinline{ocaml}|let|-expression pattern are substituted in the \mintinline{ocaml}|let|-expression's body, and the variable(s) bound during a function application are substituted into the function's body, and then the body is evaluated.

% TODO: show example of this

In this formulation, variables are ``given meaning'' via substitution; once evaluation reaches an expression, all variables in scope (in the typing context) will have been replaced by their value by some containing binding expression. In other words, variables are never evaluated directly; they are substituted by their values when bound, and their values are evaluated. The substitution model is useful for teaching purposes because it is simple and close to its mathematical definition: a variable can be thought of as an equivalent stand-in for its value.

However, for the purpose of computational efficiency, a model in which values are lazily expanded (``looked-up'') only when needed is more efficient. This is called the \textit{environment model of evaluation}, and generally is more efficient because the runtime does not need to perform an extra substitution pass over subexpressions and because untraversed (unevaluated) branches do not require substituting. Lastly, the runtime does not need to carry an expression-level IR of the language, due to the fact that the substitution model manipulates expressions, while evaluation does not. This means that the latter is more amenable for compilation, and is how compiled languages tend to be implemented: each frame of the theoretical stack frame is a de facto environment frame. While switching from the substitution to environment model is not an improvement in asymptotic efficency, these effects are useful especially for high-performance and compiled languages.

Note that the substitution model does not imply a lazy (i.e., normal-order, call-by-name, call-by-need) evaluation \cite{plotkin1975call} as in languages such as Haskell or Miranda, in which bound variables are (by default) not evaluated until their value is required. Laziness is conceptually tied to substitution, but the substitution model does not require laziness. Like most programming languages, Hazel only has strict (i.e., applicative-order, call-by-value) evaluation: the expressions bound to variables are evaluated at the time of binding.

% TODO: "Laziness is conceptually tied to substitution" -- but does it require substitution? Methinks so but not sure;
%   will find out more later due to this independent study

The implementation of evaluation with environments differs from that of evaluation with substitution primarily in that: an evaluation environment is required to look up bound variables as evaluation reaches them; binding constructs extend the evaluation environment rather than performing substitution; and $\lambda$ abstractions are bound with their evaluation environment at runtime to form (lexical) closures.

% \subsection{Approaches to programming interfaces}
% \label{sec:prog_intf}

% \subsubsection{Structure editors}
% \label{sec:structure_editors}

% \subsubsection{Graphical editors}
% \label{sec:graphical_editors}

% \subsubsection{Intentional, generative, and meta-programming}
% \label{sec:intentional_programming}

% \subsubsection{Applications to programming education}
% \label{sec:programming_education}

% \subsubsection{Drawbacks of non-textual editors}
% \label{sec:textual_benefits}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
